{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "path = 'nnUnet_raw/Dataset353_SingleChannel/labelsTr'\n",
    "\n",
    "background = 0\n",
    "myocardium = 0\n",
    "endocardium = 0\n",
    "lumen = 0\n",
    "ecm = 0\n",
    "\n",
    "folds=[[2,4,5,6],[1,2,4,5],[1,2,4,6],[1,4,5,6],[1,2,5,6]]\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.nii.gz'):\n",
    "        img = sitk.ReadImage(os.path.join(path, file))\n",
    "        arr = sitk.GetArrayFromImage(img)\n",
    "        \n",
    "        back = np.sum(arr == 0)\n",
    "        myo = np.sum(arr == 1)\n",
    "        endo = np.sum(arr == 2)\n",
    "        lum = np.sum(arr == 3)\n",
    "        jelly = np.sum(arr == 4)\n",
    "        total = back + myo + endo + lum + jelly\n",
    "        print(f\"File: {file}\")\n",
    "        print(f\"Background: {back}, Myocardium: {myo}, Endocardium: {endo}, Lumen: {lum}, ECM: {jelly}\")\n",
    "        print(f\"Background percentage: {back / total * 100:.2f}%\")\n",
    "        print(f\"Myocardium percentage: {myo / total * 100:.2f}%\")\n",
    "        print(f\"Endocardium percentage: {endo / total * 100:.2f}%\")\n",
    "        print(f\"Lumen percentage: {lum / total * 100:.2f}%\")\n",
    "        print(f\"ECM percentage: {jelly / total * 100:.2f}%\")\n",
    "        print(\"-\" * 50+\"\\n\")\n",
    "\n",
    "        background += back\n",
    "        myocardium += myo\n",
    "        endocardium += endo\n",
    "        lumen += lum\n",
    "        ecm += jelly\n",
    "\n",
    "\n",
    "print(\"Summary of Voxel Counts of all datasets (images) \\n\")\n",
    "print(f\"Background Voxels: {background}\")\n",
    "print(f\"Myocardium Voxels: {myocardium}\")\n",
    "print(f\"Endocardium Voxels: {endocardium}\")\n",
    "print(f\"Lumen Voxels: {lumen}\")\n",
    "print(f\"ECM Voxels: {ecm}\")\n",
    "\n",
    "print(\"\\nPercentage of each class in the dataset including background:\\n\")\n",
    "total = background + myocardium + endocardium + lumen + ecm\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Background percentage: {background / (total) * 100:.2f}%\")\n",
    "print(f\"Myocardium percentage: {myocardium / (total) * 100:.2f}%\")\n",
    "print(f\"Endocardium percentage: {endocardium / (total) * 100:.2f}%\")\n",
    "print(f\"Lumen percentage: {lumen / (total) * 100:.2f}%\")\n",
    "print(f\"ECM percentage: {ecm / (total) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nPercentage of each class in the dataset excluding background:\\n\")\n",
    "total_excluding_background = myocardium + endocardium + lumen + ecm\n",
    "print(f\"Total excluding background: {total_excluding_background}\")\n",
    "print(f\"Myocardium percentage: {myocardium / (total_excluding_background) * 100:.2f}%\")\n",
    "print(f\"Endocardium percentage: {endocardium / (total_excluding_background) * 100:.2f}%\")\n",
    "print(f\"Lumen percentage: {lumen / (total_excluding_background) * 100:.2f}%\")\n",
    "print(f\"ECM percentage: {ecm / (total_excluding_background) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "path = '/Users/dbattagodage/Desktop/Datasets/nnUnet_raw/Dataset347_IF/labelsTr'\n",
    "\n",
    "background = 0\n",
    "myocardium = 0\n",
    "endocardium = 0\n",
    "lumen = 0\n",
    "ecm = 0\n",
    "\n",
    "folds=[[2,4,5,3],[1,2,4,3],[1,2,3,5],[1,4,5,3],[1,2,5,4]]\n",
    "voxel_dic ={}\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.nii.gz'):\n",
    "        img = sitk.ReadImage(os.path.join(path, file))\n",
    "        arr = sitk.GetArrayFromImage(img)\n",
    "        \n",
    "        back = np.sum(arr == 0)\n",
    "        myo = np.sum(arr == 1)\n",
    "        endo = np.sum(arr == 2)\n",
    "        lum = np.sum(arr == 3)\n",
    "        jelly = np.sum(arr == 4)\n",
    "        voxel_dic[file] = {\n",
    "            \"background\": back,\n",
    "            \"myocardium\": myo,\n",
    "            \"endocardium\": endo,\n",
    "            \"lumen\": lum,\n",
    "            \"ecm\": jelly\n",
    "        }\n",
    "for fold in folds:\n",
    "    back= 0\n",
    "    myo = 0\n",
    "    endo = 0\n",
    "    lum = 0\n",
    "    jelly = 0\n",
    "    print(f\"Fold: {fold}\")\n",
    "    for keys in voxel_dic.keys():\n",
    "        if int(keys.split('_')[2][:-7]) in fold:\n",
    "            back += voxel_dic[keys][\"background\"]\n",
    "            myo += voxel_dic[keys][\"myocardium\"]\n",
    "            endo += voxel_dic[keys][\"endocardium\"]\n",
    "            lum += voxel_dic[keys][\"lumen\"]\n",
    "            jelly += voxel_dic[keys][\"ecm\"]\n",
    "    total = myo + endo + lum + jelly\n",
    "    print(f\"Background: {back}, Myocardium: {myo}, Endocardium: {endo}, Lumen: {lum}, ECM: {jelly}\")\n",
    "    \n",
    "    print(f\"Myocardium percentage: {myo / total * 100:.2f}%\")\n",
    "    print(f\"Endocardium percentage: {endo / total * 100:.2f}%\")\n",
    "    print(f\"Lumen percentage: {lum / total * 100:.2f}%\")\n",
    "    print(f\"ECM percentage: {jelly / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img1_path =\"/Users/dbattagodage/Desktop/Datasets/nnUnet_raw/Dataset353_SingleChannel/imagesTr/IF_Tr_0001_0000.nii.gz\"\n",
    "seg = \"/Users/dbattagodage/Desktop/Datasets/nnUnet_raw/Dataset353_SingleChannel/labelsTr/IF_Tr_0001.nii.gz\"\n",
    "img1 = nib.load(img1_path)\n",
    "data1 = img1.get_fdata()  # Get 3D volume as numpy array\n",
    "seg_map = nib.load(seg)\n",
    "seg_data = seg_map.get_fdata()  # Get segmentation data as numpy array\n",
    "slice_index1 = data1.shape[2] // 2 +5 # Middle slice\n",
    "xy_slice1 = data1[:, :, slice_index1]\n",
    "seg_slice1 = seg_data[:, :, slice_index1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), dpi=300)\n",
    "ax.imshow(xy_slice1.T, cmap='gray', origin='lower')\n",
    "ax.imshow(seg_slice1.T, cmap='jet', alpha=0.4, origin='lower')  # alpha=transparency\n",
    "ax.set_title(\"Image with Segmentation Overlay\")\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('overlay_segmentation.pdf', format='pdf', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved overlay image with segmentation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# File paths\n",
    "img1_path = \"/Users/dbattagodage/Desktop/Datasets/New datasets/IFTest/IF_Tr_0003_0000.nii.gz\"\n",
    "img2_path = \"/Users/dbattagodage/Desktop/Datasets/New datasets/IFTest/IF_Tr_0003_0001.nii.gz\"\n",
    "# img3_path =  \"/Users/dbattagodage/Desktop/Datasets/New datasets/IFTest/IF_Tr_0003_0002.nii.gz\"\n",
    "seg_path = \"/Users/dbattagodage/Desktop/Datasets/Model_results/IF351_out_folder/IF_Tr_0003.nii.gz\"\n",
    "# orig_seg_path =\"/Users/dbattagodage/Desktop/Datasets/Model_results/IF351_out_folder/GT/IF_Ts_0004.nii.gz\"\n",
    "\n",
    "# Load data\n",
    "img1 = nib.load(img1_path)\n",
    "data1 = img1.get_fdata().astype(np.uint8) \n",
    "\n",
    "img2 = nib.load(img2_path)\n",
    "data2 = img2.get_fdata().astype(np.uint8)  \n",
    "\n",
    "img3 = nib.load(img3_path)\n",
    "data3 = img3.get_fdata().astype(np.uint8)\n",
    "# orig_seg = nib.load(orig_seg_path)\n",
    "# orig_seg_data = orig_seg.get_fdata().astype(np.uint8) \n",
    "\n",
    "# seg_map = nib.load(seg_path)\n",
    "# seg_data = seg_map.get_fdata().astype(np.uint8)\n",
    "\n",
    "# Extract middle slice\n",
    "slice_index1 = data1.shape[2] // 2\n",
    "xy_slice1 = data1[:, :, slice_index1]\n",
    "xy_slice2 = data2[:, :, slice_index1]\n",
    "xy_slice3 = data3[:, :, slice_index1]\n",
    "# orig_slice3 = orig_seg_data[:, :, slice_index1]\n",
    "# seg_slice1 = seg_data[:, :, slice_index1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_slice1 = data1[:, :, slice_index1+40]\n",
    "xy_slice2 = data2[:, :, slice_index1+40]\n",
    "xy_slice3 = data3[:, :, slice_index1+40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    1: (\"Myocardium\", (0.89, 0.10, 0.11)),  # Crimson\n",
    "    2: (\"Endocardium\", (0.22, 0.49, 0.72)), # Steel Blue\n",
    "    3: (\"Lumen\", (0.30, 0.68, 0.29)),       # Medium Green\n",
    "    4: (\"ECM\", (0.60, 0.31, 0.64))          # Plum\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), dpi=100)\n",
    "ax.imshow(xy_slice1.T, cmap='gray', origin='lower')\n",
    "ax.imshow(xy_slice2.T, cmap='gray', alpha=0.5,origin='lower')  # Overlay second image\n",
    "ax.imshow(xy_slice3.T, cmap='gray', alpha =0.5, origin='lower')  # Overlay third image\n",
    "# ax.imshow(xy_slice3.T, cmap='gray', origin='lower')  # Overlay third image\n",
    "\n",
    "# # Overlay segmentation masks manually\n",
    "# for label_val, (label_name, color_rgb) in label_map.items():\n",
    "#     mask = (seg_slice1.T == label_val)\n",
    "#     colored_mask = np.zeros((*mask.shape, 4))  # RGBA\n",
    "#     colored_mask[..., :3] = color_rgb\n",
    "#     colored_mask[..., 3] = mask * 0.4  # Alpha\n",
    "#     ax.imshow(colored_mask, origin='lower')\n",
    "\n",
    "# # Create legend\n",
    "# legend_elements = [\n",
    "#     Patch(facecolor=color_rgb, edgecolor='black', label=label_name)\n",
    "#     for _, (label_name, color_rgb) in label_map.items()\n",
    "# ]\n",
    "# ax.legend(handles=legend_elements, loc='upper left', fontsize='small', frameon=True)\n",
    "\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('slice_1.png', format='png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved overlay image with segmentation and legend.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce78fa",
   "metadata": {},
   "source": [
    "## Image Intensity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b80f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage import exposure\n",
    "\n",
    "folder_path = \"/Users/dbattagodage/Desktop/Datasets/nnUnet_raw/Dataset352_IF/imagesTr\"\n",
    "\n",
    "def Read_image(img_number:int) -> list:\n",
    "\n",
    "    channels =[]\n",
    "    n=0\n",
    "\n",
    "    while True:\n",
    "        if os.path.isfile(os.path.join(folder_path,\"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\")):\n",
    "            img_path = os.path.join(folder_path,\"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\")\n",
    "            img = nib.load(img_path)\n",
    "            data = img.get_fdata().astype(np.uint8)\n",
    "            channels.append(data)\n",
    "        else:\n",
    "            break\n",
    "        n+=1\n",
    "    print(\"Number of channels read:\", len(channels))\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d473ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_intensities(channels_list, max_voxels=5_000_000, threshold_min=None, threshold_max=None):\n",
    "    rng = np.random.default_rng(0)\n",
    "    samples = []\n",
    "    for ch in channels_list:\n",
    "        ch = np.array(ch)\n",
    "        x = ch.ravel()\n",
    "        if threshold_min is not None: x = x[x >= threshold_min]\n",
    "        if threshold_max is not None: x = x[x <= threshold_max]\n",
    "        if x.size > 0:\n",
    "            if x.size > max_voxels:\n",
    "                idx = rng.choice(x.size, size=max_voxels, replace=False)\n",
    "                x = x[idx]\n",
    "            samples.append(x.astype(np.uint8))\n",
    "    if not samples:\n",
    "        raise ValueError(\"No data collected for pooled intensities.\")\n",
    "    return np.concatenate(samples)\n",
    "\n",
    "from skimage.filters import threshold_otsu, threshold_yen, threshold_triangle, threshold_isodata, threshold_li\n",
    "\n",
    "def thresholds_from_hist_methods(intensities):\n",
    "    return {\n",
    "        \"otsu\": threshold_otsu(intensities),\n",
    "        \"yen\": threshold_yen(intensities),\n",
    "        \"triangle\": threshold_triangle(intensities),\n",
    "        \"isodata\": threshold_isodata(intensities),\n",
    "        \"li\": threshold_li(intensities)\n",
    "    }\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def gmm_bayes_threshold(intensities, n_components=2):\n",
    "    x = intensities.reshape(-1,1)\n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=0)\n",
    "    gmm.fit(x)\n",
    "    w = gmm.weights_\n",
    "    m = gmm.means_.ravel()\n",
    "    s2 = np.array([c for c in gmm.covariances_]).reshape(-1)  # variances\n",
    "    # Solve w1 N(x|m1,s1) = w2 N(x|m2,s2); quadratic in x\n",
    "    a = 1/(2*s2[1]) - 1/(2*s2[0])\n",
    "    b = m[0]/s2[0] - m[1]/s2[1]\n",
    "    c = (m[1]**2)/(2*s2[1]) - (m[0]**2)/(2*s2[0]) + np.log((w[1]*np.sqrt(s2[0]))/(w[0]*np.sqrt(s2[1])))\n",
    "    # Handle equal-variance (a≈0) gracefully\n",
    "    if abs(a) < 1e-12:\n",
    "        t = -c/b\n",
    "        return float(t)\n",
    "    roots = np.roots([a,b,c])\n",
    "    # pick root between the means (if possible) or closest to them\n",
    "    t_candidates = [r.real for r in roots if abs(r.imag) < 1e-6]\n",
    "    if len(t_candidates)==2:\n",
    "        lo, hi = min(m), max(m)\n",
    "        between = [t for t in t_candidates if lo<=t<=hi]\n",
    "        return float(between[0] if between else t_candidates[np.argmin([abs(t-lo)+abs(t-hi) for t in t_candidates])])\n",
    "    return float(t_candidates[0])\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def gmm_bayes_threshold_1d(intensities, n_components=2,\n",
    "                           covariance_type='diag',  # 'diag' is simpler/robuster for 1-D\n",
    "                           reg_covar=1e-6,\n",
    "                           random_state=0):\n",
    "    \"\"\"\n",
    "    Return the Bayes decision boundary (equal posterior) between two GMM components.\n",
    "    Works for 1-D intensities. Falls back to a numeric grid search if needed.\n",
    "    \"\"\"\n",
    "    x = np.asarray(intensities, dtype=np.float64).reshape(-1, 1)\n",
    "    if x.size == 0:\n",
    "        raise ValueError(\"Empty intensity array.\")\n",
    "    x = x[np.isfinite(x).ravel()]  # drop NaNs/Infs\n",
    "    x = x.reshape(-1, 1)\n",
    "\n",
    "    # Fit GMM\n",
    "    gmm = GaussianMixture(n_components=n_components,\n",
    "                          covariance_type=covariance_type,\n",
    "                          reg_covar=reg_covar,\n",
    "                          random_state=random_state)\n",
    "    gmm.fit(x)\n",
    "\n",
    "    if n_components != 2:\n",
    "        raise ValueError(\"This function expects n_components=2 for a single threshold.\")\n",
    "\n",
    "    w = gmm.weights_\n",
    "    m = gmm.means_.ravel()\n",
    "\n",
    "    # Extract variances per component depending on covariance_type\n",
    "    if covariance_type == 'full':\n",
    "        # shape: (n_components, 1, 1) for 1-D -> take [0,0]\n",
    "        s2 = np.array([c[0, 0] for c in gmm.covariances_])\n",
    "    elif covariance_type == 'tied':\n",
    "        # shape: (1,1) for 1-D -> same var for both comps\n",
    "        s2 = np.array([gmm.covariances_[0, 0], gmm.covariances_[0, 0]])\n",
    "    elif covariance_type == 'diag':\n",
    "        # shape: (n_components, 1) -> ravel to (n_components,)\n",
    "        s2 = gmm.covariances_.ravel()\n",
    "    elif covariance_type == 'spherical':\n",
    "        # shape: (n_components,) already variances\n",
    "        s2 = gmm.covariances_.ravel()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported covariance_type: {covariance_type}\")\n",
    "\n",
    "    # Sort components by mean so \"0\" is the darker/background mode\n",
    "    order = np.argsort(m)\n",
    "    w, m, s2 = w[order], m[order], s2[order]\n",
    "\n",
    "    # --- Analytic solution of w1*N(x|m1,s1) = w2*N(x|m2,s2) ---\n",
    "    # Quadratic: a x^2 + b x + c = 0\n",
    "    a = 1.0/(2.0*s2[1]) - 1.0/(2.0*s2[0])\n",
    "    b = m[0]/s2[0] - m[1]/s2[1]\n",
    "    c = (m[1]**2)/(2.0*s2[1]) - (m[0]**2)/(2.0*s2[0]) + np.log((w[1]*np.sqrt(s2[0]))/(w[0]*np.sqrt(s2[1])))\n",
    "\n",
    "    t = None\n",
    "    lo, hi = m[0], m[1]\n",
    "\n",
    "    try:\n",
    "        if abs(a) < 1e-14:\n",
    "            # Equal-variance case -> linear solution\n",
    "            t = -c / b\n",
    "        else:\n",
    "            roots = np.roots([a, b, c])\n",
    "            roots = [r.real for r in roots if np.isreal(r)]\n",
    "            # Prefer the root between the means; otherwise the closest to their midpoint\n",
    "            if roots:\n",
    "                between = [r for r in roots if lo <= r <= hi]\n",
    "                t = between[0] if between else min(roots, key=lambda r: abs(r - (lo + hi)/2.0))\n",
    "    except Exception:\n",
    "        t = None\n",
    "\n",
    "    # --- Numeric fallback (no SciPy): equal posterior on a dense grid ---\n",
    "    if t is None or not np.isfinite(t):\n",
    "        gmin = np.percentile(x, 0.1)\n",
    "        gmax = np.percentile(x, 99.9)\n",
    "        grid = np.linspace(gmin, gmax, 4096).reshape(-1, 1)\n",
    "        resp = gmm.predict_proba(grid)  # responsibilities per component\n",
    "        # find where |p0 - p1| is minimal\n",
    "        idx = int(np.argmin(np.abs(resp[:, 0] - resp[:, 1])))\n",
    "        t = float(grid[idx, 0])\n",
    "\n",
    "    return float(t)\n",
    "\n",
    "\n",
    "def robust_bg_threshold(intensities_bg, alpha=1e-3):\n",
    "    # Gaussian tail ⇒ k = Φ^{-1}(1-α)\n",
    "    from scipy.stats import norm\n",
    "    k = norm.ppf(1 - alpha)  # e.g., α=1e-3 → k≈3.09\n",
    "    mu = np.mean(intensities_bg)\n",
    "    sigma = np.std(intensities_bg, ddof=1)\n",
    "    return float(mu + k*sigma)\n",
    "\n",
    "def robust_bg_threshold_mad(intensities_bg, alpha=1e-3):\n",
    "    from scipy.stats import norm\n",
    "    k = norm.ppf(1 - alpha)\n",
    "    med = np.median(intensities_bg)\n",
    "    mad = np.median(np.abs(intensities_bg - med))\n",
    "    sigma_hat = 1.4826 * mad\n",
    "    return float(med + k*sigma_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0cec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_images():\n",
    "    img_numbers= [1,2,3,4,5,6]\n",
    "    all_images = []\n",
    "    for num in img_numbers:\n",
    "        channels = Read_image(num)\n",
    "        all_images.extend(channels)\n",
    "    return all_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a5d7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels read: 3\n",
      "Number of channels read: 3\n",
      "Number of channels read: 3\n",
      "Number of channels read: 3\n",
      "Number of channels read: 3\n",
      "Number of channels read: 3\n"
     ]
    }
   ],
   "source": [
    "all_images = read_all_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be33e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled = pooled_intensities(all_images, max_voxels=10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00eaa7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = thresholds_from_hist_methods(pooled)\n",
    "tau_otsu = taus[\"otsu\"]\n",
    "tau_yen = taus[\"yen\"]\n",
    "tau_triangle = taus[\"triangle\"]\n",
    "\n",
    "# or GMM:\n",
    "tau_gmm = gmm_bayes_threshold_1d(pooled,covariance_type='diag')\n",
    "\n",
    "# or background-anchored (need background samples):\n",
    "# bg = pooled_from_background_rois(...)\n",
    "# tau_bg = robust_bg_threshold(bg, alpha=1e-3)\n",
    "\n",
    "# 4) choose one τ (example: τ = tau_yen)\n",
    "tau = tau_yen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576d1a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'otsu': 111,\n",
       " 'yen': 11,\n",
       " 'triangle': 8,\n",
       " 'isodata': 111,\n",
       " 'li': 42.39745011937387}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a145ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.043956043956044, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_gmm, tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3e38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_name = [\"Endocardium\", \"Myocardium\", \"Nuclei\"]\n",
    "def plot_histogram(channels: list, threshold:int=50):\n",
    "    flattened_channels = [ch.flatten()[ch.flatten() > threshold] for ch in channels]\n",
    "    fig, axs = plt.subplots(len(flattened_channels), 1, figsize=(10, 6))\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.hist(flattened_channels[i], bins=256-threshold-1, alpha=0.7)\n",
    "        ax.set_title(channel_name[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e564e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(Read_image(1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad43557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_mkdir(folder_name:str)->str:\n",
    "    if not os.path.exists(os.path.join(folder_path, folder_name)):\n",
    "        os.makedirs(os.path.join(folder_path, folder_name))\n",
    "\n",
    "def recreate_img(img_number: int, threshold:int):\n",
    "    maybe_mkdir(f\"Thresholded_{threshold}\")\n",
    "    n=0\n",
    "    while True:\n",
    "        if os.path.isfile(os.path.join(folder_path,\"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\")):\n",
    "            img_path = os.path.join(folder_path,\"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\")\n",
    "            img = nib.load(img_path)\n",
    "            data = img.get_fdata().astype(np.uint8)\n",
    "            data[data < threshold] = 0\n",
    "            data = nib.Nifti1Image(data, img.affine, img.header)\n",
    "            nib.save(data, os.path.join(folder_path, f\"Thresholded_{threshold}\", \n",
    "                                        \"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\"))\n",
    "\n",
    "        else:\n",
    "            break\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe81d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_img(1,43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3218ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_equalization(channels: list, method:str=\"global\"):\n",
    "    equalized_channels = []\n",
    "    for ch in channels:\n",
    "        # flatten -> equalize -> reshape back\n",
    "        if method == \"global\":\n",
    "            eq = exposure.equalize_hist(ch)  # global hist eq (range [0,1])\n",
    "        elif method == \"clahe\":\n",
    "            eq = exposure.equalize_adapthist(ch, clip_limit=0.03)  # CLAHE\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'global' or 'clahe'\")\n",
    "        equalized_channels.append(eq)\n",
    "    return equalized_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3351991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels read: 3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "channels = Read_image(1)\n",
    "\n",
    "# Plot original histogram\n",
    "# plot_histogram(channels, -1)\n",
    "\n",
    "# Apply histogram equalization\n",
    "eq_channels = histogram_equalization(channels, method=\"global\")  # try \"global\" or \"clahe\"\n",
    "\n",
    "# Plot equalized histogram\n",
    "plot_histogram(eq_channels, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
