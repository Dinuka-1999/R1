{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be456fa",
   "metadata": {},
   "source": [
    "## Analyse the class percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "path = 'nnUnet_raw/Dataset353_SingleChannel/labelsTr'\n",
    "\n",
    "background = 0\n",
    "myocardium = 0\n",
    "endocardium = 0\n",
    "lumen = 0\n",
    "ecm = 0\n",
    "\n",
    "folds=[[2,4,5,6],[1,2,4,5],[1,2,4,6],[1,4,5,6],[1,2,5,6]]\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.nii.gz'):\n",
    "        img = sitk.ReadImage(os.path.join(path, file))\n",
    "        arr = sitk.GetArrayFromImage(img)\n",
    "        \n",
    "        back = np.sum(arr == 0)\n",
    "        myo = np.sum(arr == 1)\n",
    "        endo = np.sum(arr == 2)\n",
    "        lum = np.sum(arr == 3)\n",
    "        jelly = np.sum(arr == 4)\n",
    "        total = back + myo + endo + lum + jelly\n",
    "        print(f\"File: {file}\")\n",
    "        print(f\"Background: {back}, Myocardium: {myo}, Endocardium: {endo}, Lumen: {lum}, ECM: {jelly}\")\n",
    "        print(f\"Background percentage: {back / total * 100:.2f}%\")\n",
    "        print(f\"Myocardium percentage: {myo / total * 100:.2f}%\")\n",
    "        print(f\"Endocardium percentage: {endo / total * 100:.2f}%\")\n",
    "        print(f\"Lumen percentage: {lum / total * 100:.2f}%\")\n",
    "        print(f\"ECM percentage: {jelly / total * 100:.2f}%\")\n",
    "        print(\"-\" * 50+\"\\n\")\n",
    "\n",
    "        background += back\n",
    "        myocardium += myo\n",
    "        endocardium += endo\n",
    "        lumen += lum\n",
    "        ecm += jelly\n",
    "\n",
    "\n",
    "print(\"Summary of Voxel Counts of all datasets (images) \\n\")\n",
    "print(f\"Background Voxels: {background}\")\n",
    "print(f\"Myocardium Voxels: {myocardium}\")\n",
    "print(f\"Endocardium Voxels: {endocardium}\")\n",
    "print(f\"Lumen Voxels: {lumen}\")\n",
    "print(f\"ECM Voxels: {ecm}\")\n",
    "\n",
    "print(\"\\nPercentage of each class in the dataset including background:\\n\")\n",
    "total = background + myocardium + endocardium + lumen + ecm\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Background percentage: {background / (total) * 100:.2f}%\")\n",
    "print(f\"Myocardium percentage: {myocardium / (total) * 100:.2f}%\")\n",
    "print(f\"Endocardium percentage: {endocardium / (total) * 100:.2f}%\")\n",
    "print(f\"Lumen percentage: {lumen / (total) * 100:.2f}%\")\n",
    "print(f\"ECM percentage: {ecm / (total) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nPercentage of each class in the dataset excluding background:\\n\")\n",
    "total_excluding_background = myocardium + endocardium + lumen + ecm\n",
    "print(f\"Total excluding background: {total_excluding_background}\")\n",
    "print(f\"Myocardium percentage: {myocardium / (total_excluding_background) * 100:.2f}%\")\n",
    "print(f\"Endocardium percentage: {endocardium / (total_excluding_background) * 100:.2f}%\")\n",
    "print(f\"Lumen percentage: {lumen / (total_excluding_background) * 100:.2f}%\")\n",
    "print(f\"ECM percentage: {ecm / (total_excluding_background) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "path = '/Users/dbattagodage/Desktop/Datasets/nnUnet_raw/Dataset347_IF/labelsTr'\n",
    "\n",
    "background = 0\n",
    "myocardium = 0\n",
    "endocardium = 0\n",
    "lumen = 0\n",
    "ecm = 0\n",
    "\n",
    "folds=[[2,4,5,3],[1,2,4,3],[1,2,3,5],[1,4,5,3],[1,2,5,4]]\n",
    "voxel_dic ={}\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.nii.gz'):\n",
    "        img = sitk.ReadImage(os.path.join(path, file))\n",
    "        arr = sitk.GetArrayFromImage(img)\n",
    "        \n",
    "        back = np.sum(arr == 0)\n",
    "        myo = np.sum(arr == 1)\n",
    "        endo = np.sum(arr == 2)\n",
    "        lum = np.sum(arr == 3)\n",
    "        jelly = np.sum(arr == 4)\n",
    "        voxel_dic[file] = {\n",
    "            \"background\": back,\n",
    "            \"myocardium\": myo,\n",
    "            \"endocardium\": endo,\n",
    "            \"lumen\": lum,\n",
    "            \"ecm\": jelly\n",
    "        }\n",
    "for fold in folds:\n",
    "    back= 0\n",
    "    myo = 0\n",
    "    endo = 0\n",
    "    lum = 0\n",
    "    jelly = 0\n",
    "    print(f\"Fold: {fold}\")\n",
    "    for keys in voxel_dic.keys():\n",
    "        if int(keys.split('_')[2][:-7]) in fold:\n",
    "            back += voxel_dic[keys][\"background\"]\n",
    "            myo += voxel_dic[keys][\"myocardium\"]\n",
    "            endo += voxel_dic[keys][\"endocardium\"]\n",
    "            lum += voxel_dic[keys][\"lumen\"]\n",
    "            jelly += voxel_dic[keys][\"ecm\"]\n",
    "    total = myo + endo + lum + jelly\n",
    "    print(f\"Background: {back}, Myocardium: {myo}, Endocardium: {endo}, Lumen: {lum}, ECM: {jelly}\")\n",
    "    \n",
    "    print(f\"Myocardium percentage: {myo / total * 100:.2f}%\")\n",
    "    print(f\"Endocardium percentage: {endo / total * 100:.2f}%\")\n",
    "    print(f\"Lumen percentage: {lum / total * 100:.2f}%\")\n",
    "    print(f\"ECM percentage: {jelly / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85e6e1",
   "metadata": {},
   "source": [
    "### Segmentation results plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4a17f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "jason_path = \"/home/cellsmb/Desktop/Dinuka/Image_Analysis/Model_results/Model_results/IF_352_6img_results/all_segs_bg/Evaluation_summary_perslice.json\"\n",
    "folder_pred = \"/home/cellsmb/Desktop/Dinuka/Image_Analysis/Model_results/Model_results/IF_352_6img_results/all_segs_bg\"\n",
    "\n",
    "with open(jason_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "matric_per_case = data['metric_per_case']\n",
    "\n",
    "label_names = ['background',\"Myocardium\",\"Endocardium\",\"Lumen\",\"ECM\"]\n",
    "start_end = {'1':[23,549],'2':[0,697],'3':[21,317],'4':[30,375],'5':[13,367],'6':[19,595]}\n",
    "\n",
    "for ID,label in enumerate([1,2,3,4,5]):\n",
    "    Dice=[]\n",
    "    IoU=[]\n",
    "    max_length =0\n",
    "    img_IDs=[]\n",
    "    for imgs in matric_per_case:\n",
    "        Dice.append(imgs['metrics'][str(label)]['Dice_through_z_axis'])\n",
    "        IoU.append(imgs['metrics'][str(label)]['IoU_through_z_axis'])\n",
    "        max_length = max(max_length,len(imgs['metrics'][str(label)]['Dice_through_z_axis']))\n",
    "        img_IDs.append(imgs['reference_file'].split(\"/\")[-1].split(\"_\")[2].split(\".\")[0])\n",
    "    \n",
    "    fig1,ax1 = plt.subplots(figsize=(15,10))\n",
    "    fig2,ax2 = plt.subplots(figsize=(15,10))\n",
    "    \n",
    "    for r in range(len(Dice)):\n",
    "        n = len(Dice[r])\n",
    "        # offset = (max_length - n) // 2\n",
    "        # x = np.arange(offset, offset + n)  # center the shorter curve\n",
    "        x = np.linspace(0, 1, n)  \n",
    "        ax1.plot(x,Dice[r], marker='.',label=f'Image {img_IDs[r]}')\n",
    "        ax1.scatter([start_end[str(int(img_IDs[r]))][0]/n, start_end[str(int(img_IDs[r]))][1]/n], \\\n",
    "                    [Dice[r][start_end[str(int(img_IDs[r]))][0]],Dice[r][start_end[str(int(img_IDs[r]))][1]]], color='black', s=100)\n",
    "        \n",
    "        ax2.plot(x,IoU[r], marker='.',label=f'Image {img_IDs[r]}')\n",
    "        ax2.scatter([start_end[str(int(img_IDs[r]))][0], start_end[str(int(img_IDs[r]))][1]], \\\n",
    "                    [IoU[r][start_end[str(int(img_IDs[r]))][0]],IoU[r][start_end[str(int(img_IDs[r]))][1]]], color='black', s=100)\n",
    "    ax1.set_title(f'Dice through z-axis for {label_names[ID]}')\n",
    "    ax1.set_xlabel('Z-axis (slices)')\n",
    "    ax1.set_ylabel('Dice Score')\n",
    "    ax1.legend()\n",
    "    ax2.set_title(f'IoU through z-axis for {label_names[ID]}')\n",
    "    ax2.set_xlabel('Z-axis (slices)')\n",
    "    ax2.set_ylabel('IoU Score')\n",
    "    ax2.legend()\n",
    "    fig1.savefig(os.path.join(folder_pred,\"Eval_plots\",f'All_Dice_IoU_through_z_axis_label_{label}_normalised_x.png'))\n",
    "    fig2.savefig(os.path.join(folder_pred,\"Eval_plots\",f'All_IoU_through_z_axis_label_{label}_normalised_x.png'))\n",
    "    plt.close(fig1)\n",
    "    plt.close(fig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d93c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IDEAL plots\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "folder_pred = \"/Users/dbattagodage/Desktop/Datasets/Model_results/IF_352_6img_results/all_segs_bg\"\n",
    "label_names = ['background',\"Myocardium\",\"Endocardium\",\"Lumen\",\"ECM\"]\n",
    "start_end = {'1':[23,549],'2':[0,697],'3':[21,317],'4':[30,375],'5':[13,367],'6':[19,595]}\n",
    "json_path = \"/Users/dbattagodage/Desktop/Datasets/Model_results/IF_352_6img_results/all_segs_bg/Evaluation_summary_perslice.json\"\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    results = json.load(f)['metric_per_case']\n",
    "\n",
    "\n",
    "Dice=[]\n",
    "img_IDs=[]\n",
    "max_length=0\n",
    "for imgs in results:\n",
    "    Dice.append(imgs['metrics'][\"1\"]['Dice_through_z_axis'])\n",
    "    img_IDs.append(imgs['reference_file'].split(\"/\")[-1].split(\"_\")[2].split(\".\")[0])\n",
    "    max_length = max(max_length, len(Dice[-1]))\n",
    "\n",
    "fig1,ax1 = plt.subplots(figsize=(15,10))\n",
    "fig2,ax2 = plt.subplots(figsize=(15,10))\n",
    "for r in range(len(Dice)):\n",
    "    n = len(Dice[r])\n",
    "    x = np.linspace(0, 1, n)  \n",
    "    y = np.zeros(n)\n",
    "    start_end_values = start_end[str(int(img_IDs[r]))]\n",
    "    y[start_end_values[0]:start_end_values[1]+1] = 1\n",
    "    ax1.plot(x,y, label=f\"Image {img_IDs[r]}\")\n",
    "\n",
    "    offset = (max_length - n) // 2\n",
    "    x = np.arange(offset, offset + n)  # center the shorter curve\n",
    "    ax2.plot(x,y, label=f\"Image {img_IDs[r]}\")\n",
    "\n",
    "ax1.set_title('Ideal Situation for all tissues')\n",
    "ax1.set_xlabel('Z-axis (slices)')\n",
    "ax1.set_ylabel('Tissue Presence (1=Present, 0=Absent)')\n",
    "ax1.legend()\n",
    "fig1.savefig(os.path.join(folder_pred,\"Eval_plots\",'Ideal_plots_normalised_x.png'))\n",
    "\n",
    "ax2.set_title('Ideal Situation for all tissues')\n",
    "ax2.set_xlabel('Z-axis (slices)')\n",
    "ax2.set_ylabel('Tissue Presence (1=Present, 0=Absent)')\n",
    "ax2.legend()\n",
    "fig2.savefig(os.path.join(folder_pred,\"Eval_plots\",'Ideal_plots_.png'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833cef8",
   "metadata": {},
   "source": [
    "## Evaluation matrices analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999eb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "json_path = \"/Users/dbattagodage/Desktop/Datasets/Model_results/IF_352_6img_results/nnUNetTrainerUMambaEncNoAMP__nnUNetPlans__3d_fullres/\"\n",
    "\n",
    "class_mapping = [\"Myocardium\", \"Endocardium\", \"Lumen\", \"ECM\"]\n",
    "def find_metric_heatmap(json_path, just_foreground=True, only_dice_and_iou=False):\n",
    "    metrics_list = []\n",
    "    case = []\n",
    "    for folds in os.listdir(json_path):\n",
    "        if os.path.isdir(os.path.join(json_path, folds)):\n",
    "            jason_file_path = os.path.join(json_path, folds, 'validation_no_bg', 'summary.json')\n",
    "\n",
    "            with open(jason_file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            metric_class=data['metric_per_case']\n",
    "            for r in range(len(metric_class)):\n",
    "                case_id = metric_class[r][\"reference_file\"].split('/')[-1].split('_')[2][:-7]\n",
    "                if not just_foreground:\n",
    "                    for i in range(1, 5):\n",
    "                        TP = metric_class[r][\"metrics\"][str(i)][\"TP\"]\n",
    "                        TN = metric_class[r][\"metrics\"][str(i)][\"TN\"]\n",
    "                        FP = metric_class[r][\"metrics\"][str(i)][\"FP\"]\n",
    "                        FN = metric_class[r][\"metrics\"][str(i)][\"FN\"]\n",
    "\n",
    "                        dice = metric_class[r][\"metrics\"][str(i)][\"Dice\"]\n",
    "                        iou = metric_class[r][\"metrics\"][str(i)][\"IoU\"]\n",
    "\n",
    "                        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "                        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "                        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "                        metrics_list.append({\n",
    "                            \"Case\": case_id,\n",
    "                            \"Metric\": f\"{class_mapping[i-1]}_Dice\", \"Value\": dice\n",
    "                        })\n",
    "                        metrics_list.append({\n",
    "                            \"Case\": case_id,\n",
    "                            \"Metric\": f\"{class_mapping[i-1]}_IoU\", \"Value\": iou\n",
    "                        })\n",
    "                        if not only_dice_and_iou:\n",
    "                            metrics_list.append({\n",
    "                                \"Case\": case_id,\n",
    "                                \"Metric\": f\"{class_mapping[i-1]}_Precision\", \"Value\": precision\n",
    "                            })\n",
    "                            metrics_list.append({\n",
    "                                \"Case\": case_id,\n",
    "                                \"Metric\": f\"{class_mapping[i-1]}_Recall\", \"Value\": recall\n",
    "                            })\n",
    "                            metrics_list.append({\n",
    "                                \"Case\": case_id,\n",
    "                                \"Metric\": f\"{class_mapping[i-1]}_Accuracy\", \"Value\": accuracy\n",
    "                            })\n",
    "                else:\n",
    "                    TP = sum([metric_class[r][\"metrics\"][str(i)][\"TP\"] for i in range(1,5)])\n",
    "                    TN = sum([metric_class[r][\"metrics\"][str(i)][\"TN\"] for i in range(1,5)])\n",
    "                    FP = sum([metric_class[r][\"metrics\"][str(i)][\"FP\"] for i in range(1,5)])\n",
    "                    FN = sum([metric_class[r][\"metrics\"][str(i)][\"FN\"] for i in range(1,5)])\n",
    "\n",
    "                    dice = np.mean([metric_class[r][\"metrics\"][str(i)][\"Dice\"] for i in range(1,5)])\n",
    "                    iou =  np.mean([metric_class[r][\"metrics\"][str(i)][\"IoU\"] for i in range(1,5)])\n",
    "\n",
    "                    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "                    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "                    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "                    metrics_list.append({\n",
    "                            \"Case\": case_id,\n",
    "                            \"Metric\": \"Dice\", \"Value\": dice\n",
    "                        })\n",
    "                    metrics_list.append({\n",
    "                        \"Case\": case_id,\n",
    "                        \"Metric\": \"IoU\", \"Value\": iou\n",
    "                    })\n",
    "                    if not only_dice_and_iou:\n",
    "                        metrics_list.append({\n",
    "                            \"Case\": case_id,\n",
    "                            \"Metric\": \"Precision\", \"Value\": precision\n",
    "                        })\n",
    "                        metrics_list.append({\n",
    "                            \"Case\": case_id,\n",
    "                            \"Metric\": \"Recall\", \"Value\": recall\n",
    "                        })\n",
    "                        metrics_list.append({\n",
    "                            \"Case\": case_id,\n",
    "                            \"Metric\": \"Accuracy\", \"Value\": accuracy\n",
    "                        })\n",
    "    return metrics_list, case\n",
    "\n",
    "metrics_list, case = find_metric_heatmap(json_path, just_foreground=True, only_dice_and_iou=False)\n",
    "df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Pivot: rows = metrics, cols = cases, values = Value\n",
    "df_pivot = df.pivot(index=\"Metric\", columns=\"Case\", values=\"Value\")\n",
    "\n",
    "# Sort columns\n",
    "df_pivot = df_pivot.sort_index(axis=1)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df_pivot, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.xlabel(\"Cases\")\n",
    "plt.title(\"Evaluation Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce78fa",
   "metadata": {},
   "source": [
    "## Image Intensity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c34564",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For over slices\n",
    "from importlib.metadata import files\n",
    "import nibabel as nib\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "folder = \"/Users/dbattagodage/Desktop/Datasets/nnUnet_raw/Dataset352_IF/imagesTr\"\n",
    "destination = \"/Users/dbattagodage/Desktop/Datasets/Image_analysis/Intensity_plots/new_testing\"\n",
    "segmentation_folder = \"/Users/dbattagodage/Desktop/Datasets/nnUnet_raw/Dataset352_IF/labelsTr\"\n",
    "\n",
    "file_List=os.listdir(folder)\n",
    "file_List.sort()\n",
    "already_done = []\n",
    "\n",
    "def read_file(file):\n",
    "    global already_done\n",
    "    img_arrays = []\n",
    "    if file.endswith('.nii.gz'):\n",
    "        if file.split(\"_\")[2] not in already_done:\n",
    "            for r in range(3):\n",
    "                img_path = os.path.join(folder,f\"IF_Tr_{file.split('_')[2]}_{r:04d}.nii.gz\")\n",
    "                img = nib.load(img_path)\n",
    "                arr = img.get_fdata().astype(np.uint8)\n",
    "                img_arrays.append(arr)\n",
    "        already_done.append(file.split(\"_\")[2])\n",
    "    return img_arrays\n",
    "        \n",
    "\n",
    "def plot_intensity_profiles(images, files, volume:bool=False,seg:bool=False):\n",
    "    image_type = {\"0000\":\"Endocardium\",\"0001\":\"Myocardium\",\"0002\":\"Nuclei\"}\n",
    "    if volume:\n",
    "        value_dict = {\"0\":{'mean':[], 'std':[]}, \"1\":{'mean':[], 'std':[]}, \"2\":{'mean':[], 'std':[]}}\n",
    "        fig, ax =plt.subplots(1,3, figsize=(15,5))\n",
    "        for file in images:\n",
    "            arrs = read_file(file)\n",
    "            if arrs:\n",
    "                if seg:\n",
    "                    seg_arr = nib.load(os.path.join(segmentation_folder,f\"IF_Tr_{file.split('_')[2]}.nii.gz\"))\n",
    "                    seg_arr = seg_arr.get_fdata().astype(np.uint8)\n",
    "                    foreground_mask = seg_arr > 0\n",
    "                    for r in range(3):\n",
    "                        arr = arrs[r] * foreground_mask\n",
    "                        new = np.where(arr>0,arr,np.nan)\n",
    "                        mean = np.nanmean(new)\n",
    "                        std = np.nanstd(new)\n",
    "                        value_dict[f\"{r}\"]['mean'].append(mean)\n",
    "                        value_dict[f\"{r}\"]['std'].append(std)\n",
    "                else:\n",
    "                    for r in range(3):\n",
    "                        mean = np.mean(arrs[r])\n",
    "                        std = np.std(arrs[r])\n",
    "                        value_dict[f\"{r}\"]['mean'].append(mean)\n",
    "                        value_dict[f\"{r}\"]['std'].append(std)\n",
    "        #candle ticks for the mean and std\n",
    "        for r in range(3):\n",
    "            ax[r].errorbar(np.arange(len(value_dict[f\"{r}\"]['mean'])), value_dict[f\"{r}\"]['mean'], yerr=value_dict[f\"{r}\"]['std'], label=image_type[f\"{r:04d}\"], capsize=5)\n",
    "            ax[r].set_title(f\"{image_type[f'{r:04d}']}\", fontsize=14)\n",
    "            ax[r].set_xlabel(\"Image Index\", fontsize=14)\n",
    "            ax[r].set_ylabel(\"Intensity\", fontsize=14)\n",
    "            ax[r].grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(os.path.join(destination,f\"Intensity_plot_Volume_{files.split('_')[2]}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    else:\n",
    "        fig, ax = plt.subplots(3,3, figsize=(15,15))\n",
    "\n",
    "        for r in range(3):\n",
    "            planes = {\"XY\": images[0].shape[2],\"XZ\": images[0].shape[1],\"YZ\": images[0].shape[0]}\n",
    "            for c,key in enumerate(planes.keys()):                    \n",
    "                reshaped = images[r].reshape(-1, planes[key])\n",
    "                if seg:\n",
    "                    new = np.where(reshaped>0,reshaped,np.nan)\n",
    "                    mean = np.nanmean(new, axis=0)\n",
    "                    std = np.nanstd(new, axis=0)\n",
    "                    mean  = np.nan_to_num(mean, nan =0.0)\n",
    "                    std = np.nan_to_num(std, nan=0.0)\n",
    "                else:\n",
    "                    mean = np.mean(reshaped, axis=0)\n",
    "                    std = np.std(reshaped, axis=0)\n",
    "                ax[r, c].plot(mean,label=\"Mean intensity\",color='blue')\n",
    "                ax[r, c].fill_between(range(len(mean)), mean - std, mean + std, color='blue', alpha=0.2,label='±1 Std Dev')\n",
    "                ax[r, c].grid()\n",
    "                if r == 0:\n",
    "                    ax[r, c].set_title(f\"{key} plane\", fontsize=14)\n",
    "            ax[r, 0].set_ylabel(image_type[f\"{r:04d}\"], fontsize=14)\n",
    "        ax[2, 0].set_xlabel(\"Z depth\", fontsize=14)\n",
    "        ax[2, 1].set_xlabel(\"Y depth\", fontsize=14)\n",
    "        ax[2, 2].set_xlabel(\"X depth\", fontsize=14)\n",
    "        fig.text(0.04, 0.5, \"Intensity\", va=\"center\", rotation=\"vertical\", fontsize=14)\n",
    "\n",
    "        # Shared legend\n",
    "        handles, labels = ax[0, 0].get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc=\"upper right\", fontsize=12)\n",
    "\n",
    "        # Super title\n",
    "        fig.suptitle(f\"Mean Intensity Profiles Across Planes of {files.split('_')[2]}\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n",
    "        plt.show()\n",
    "        plt.savefig(os.path.join(destination,f\"Intensity_plot_{files.split('_')[2]}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "def intensity_profiles(volume:bool=False, only_foreground:bool=False):\n",
    "\n",
    "    if not only_foreground and not volume:\n",
    "        for file in file_List:\n",
    "            arrs = read_file(file)\n",
    "            if arrs:\n",
    "                plot_intensity_profiles(arrs, files=file, volume=volume)\n",
    "\n",
    "    elif volume and not only_foreground:\n",
    "        plot_intensity_profiles(file_List, file_List, volume=volume)\n",
    "\n",
    "    elif not volume and only_foreground:\n",
    "        if segmentation_folder is None:\n",
    "            raise ValueError(\"Segmentation folder must be provided when only_foreground is True.\")\n",
    "        else:\n",
    "            for file in file_List:\n",
    "                arrs = read_file(file)\n",
    "                seg_arr = nib.load(os.path.join(segmentation_folder,f\"IF_Tr_{file.split('_')[2]}.nii.gz\"))\n",
    "                seg_arr = seg_arr.get_fdata().astype(np.uint8)\n",
    "                foreground_mask = seg_arr > 0\n",
    "                if arrs:\n",
    "                    for idx, arr in enumerate(arrs):\n",
    "                        arr = arr * foreground_mask\n",
    "                        plot_intensity_profiles(arr, files=file, volume=volume,seg=True)\n",
    "    else:\n",
    "        if segmentation_folder is None:\n",
    "            raise ValueError(\"Segmentation folder must be provided when only_foreground is True.\")\n",
    "        else:\n",
    "            plot_intensity_profiles(file_List, file_List, volume=volume,seg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e291359",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_profiles(volume=True, only_foreground=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b80f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage import exposure\n",
    "\n",
    "folder_path = \"/home/cellsmb/Desktop/Dinuka/Image_Analysis/nnUnet_raw/imagesTr\"\n",
    "\n",
    "def Read_image(img_number:int) -> list:\n",
    "\n",
    "    channels =[]\n",
    "    n=0\n",
    "\n",
    "    while True:\n",
    "        if os.path.isfile(os.path.join(folder_path,\"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\")):\n",
    "            img_path = os.path.join(folder_path,\"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\")\n",
    "            img = nib.load(img_path)\n",
    "            data = img.get_fdata().astype(np.uint8)\n",
    "            channels.append(data)\n",
    "        else:\n",
    "            break\n",
    "        n+=1\n",
    "    print(\"Number of channels read:\", len(channels))\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d473ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_intensities(channels_list, max_voxels=5_000_000, threshold_min=None, threshold_max=None):\n",
    "    rng = np.random.default_rng(0)\n",
    "    samples = []\n",
    "    for ch in channels_list:\n",
    "        ch = np.array(ch)\n",
    "        x = ch.ravel()\n",
    "        if threshold_min is not None: x = x[x >= threshold_min]\n",
    "        if threshold_max is not None: x = x[x <= threshold_max]\n",
    "        if x.size > 0:\n",
    "            if x.size > max_voxels:\n",
    "                idx = rng.choice(x.size, size=max_voxels, replace=False)\n",
    "                x = x[idx]\n",
    "            samples.append(x.astype(np.uint8))\n",
    "    if not samples:\n",
    "        raise ValueError(\"No data collected for pooled intensities.\")\n",
    "    return np.concatenate(samples)\n",
    "\n",
    "from skimage.filters import threshold_otsu, threshold_yen, threshold_triangle, threshold_isodata, threshold_li\n",
    "\n",
    "def thresholds_from_hist_methods(intensities):\n",
    "    return {\n",
    "        \"otsu\": threshold_otsu(intensities),\n",
    "        \"yen\": threshold_yen(intensities),\n",
    "        \"triangle\": threshold_triangle(intensities),\n",
    "        \"isodata\": threshold_isodata(intensities),\n",
    "        \"li\": threshold_li(intensities)\n",
    "    }\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def gmm_bayes_threshold(intensities, n_components=2):\n",
    "    x = intensities.reshape(-1,1)\n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=0)\n",
    "    gmm.fit(x)\n",
    "    w = gmm.weights_\n",
    "    m = gmm.means_.ravel()\n",
    "    s2 = np.array([c for c in gmm.covariances_]).reshape(-1)  # variances\n",
    "    # Solve w1 N(x|m1,s1) = w2 N(x|m2,s2); quadratic in x\n",
    "    a = 1/(2*s2[1]) - 1/(2*s2[0])\n",
    "    b = m[0]/s2[0] - m[1]/s2[1]\n",
    "    c = (m[1]**2)/(2*s2[1]) - (m[0]**2)/(2*s2[0]) + np.log((w[1]*np.sqrt(s2[0]))/(w[0]*np.sqrt(s2[1])))\n",
    "    # Handle equal-variance (a≈0) gracefully\n",
    "    if abs(a) < 1e-12:\n",
    "        t = -c/b\n",
    "        return float(t)\n",
    "    roots = np.roots([a,b,c])\n",
    "    # pick root between the means (if possible) or closest to them\n",
    "    t_candidates = [r.real for r in roots if abs(r.imag) < 1e-6]\n",
    "    if len(t_candidates)==2:\n",
    "        lo, hi = min(m), max(m)\n",
    "        between = [t for t in t_candidates if lo<=t<=hi]\n",
    "        return float(between[0] if between else t_candidates[np.argmin([abs(t-lo)+abs(t-hi) for t in t_candidates])])\n",
    "    return float(t_candidates[0])\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def gmm_bayes_threshold_1d(intensities, n_components=2,\n",
    "                           covariance_type='diag',  # 'diag' is simpler/robuster for 1-D\n",
    "                           reg_covar=1e-6,\n",
    "                           random_state=0):\n",
    "    \"\"\"\n",
    "    Return the Bayes decision boundary (equal posterior) between two GMM components.\n",
    "    Works for 1-D intensities. Falls back to a numeric grid search if needed.\n",
    "    \"\"\"\n",
    "    x = np.asarray(intensities, dtype=np.float64).reshape(-1, 1)\n",
    "    if x.size == 0:\n",
    "        raise ValueError(\"Empty intensity array.\")\n",
    "    x = x[np.isfinite(x).ravel()]  # drop NaNs/Infs\n",
    "    x = x.reshape(-1, 1)\n",
    "\n",
    "    # Fit GMM\n",
    "    gmm = GaussianMixture(n_components=n_components,\n",
    "                          covariance_type=covariance_type,\n",
    "                          reg_covar=reg_covar,\n",
    "                          random_state=random_state)\n",
    "    gmm.fit(x)\n",
    "\n",
    "    if n_components != 2:\n",
    "        raise ValueError(\"This function expects n_components=2 for a single threshold.\")\n",
    "\n",
    "    w = gmm.weights_\n",
    "    m = gmm.means_.ravel()\n",
    "\n",
    "    # Extract variances per component depending on covariance_type\n",
    "    if covariance_type == 'full':\n",
    "        # shape: (n_components, 1, 1) for 1-D -> take [0,0]\n",
    "        s2 = np.array([c[0, 0] for c in gmm.covariances_])\n",
    "    elif covariance_type == 'tied':\n",
    "        # shape: (1,1) for 1-D -> same var for both comps\n",
    "        s2 = np.array([gmm.covariances_[0, 0], gmm.covariances_[0, 0]])\n",
    "    elif covariance_type == 'diag':\n",
    "        # shape: (n_components, 1) -> ravel to (n_components,)\n",
    "        s2 = gmm.covariances_.ravel()\n",
    "    elif covariance_type == 'spherical':\n",
    "        # shape: (n_components,) already variances\n",
    "        s2 = gmm.covariances_.ravel()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported covariance_type: {covariance_type}\")\n",
    "\n",
    "    # Sort components by mean so \"0\" is the darker/background mode\n",
    "    order = np.argsort(m)\n",
    "    w, m, s2 = w[order], m[order], s2[order]\n",
    "\n",
    "    # --- Analytic solution of w1*N(x|m1,s1) = w2*N(x|m2,s2) ---\n",
    "    # Quadratic: a x^2 + b x + c = 0\n",
    "    a = 1.0/(2.0*s2[1]) - 1.0/(2.0*s2[0])\n",
    "    b = m[0]/s2[0] - m[1]/s2[1]\n",
    "    c = (m[1]**2)/(2.0*s2[1]) - (m[0]**2)/(2.0*s2[0]) + np.log((w[1]*np.sqrt(s2[0]))/(w[0]*np.sqrt(s2[1])))\n",
    "\n",
    "    t = None\n",
    "    lo, hi = m[0], m[1]\n",
    "\n",
    "    try:\n",
    "        if abs(a) < 1e-14:\n",
    "            # Equal-variance case -> linear solution\n",
    "            t = -c / b\n",
    "        else:\n",
    "            roots = np.roots([a, b, c])\n",
    "            roots = [r.real for r in roots if np.isreal(r)]\n",
    "            # Prefer the root between the means; otherwise the closest to their midpoint\n",
    "            if roots:\n",
    "                between = [r for r in roots if lo <= r <= hi]\n",
    "                t = between[0] if between else min(roots, key=lambda r: abs(r - (lo + hi)/2.0))\n",
    "    except Exception:\n",
    "        t = None\n",
    "\n",
    "    # --- Numeric fallback (no SciPy): equal posterior on a dense grid ---\n",
    "    if t is None or not np.isfinite(t):\n",
    "        gmin = np.percentile(x, 0.1)\n",
    "        gmax = np.percentile(x, 99.9)\n",
    "        grid = np.linspace(gmin, gmax, 4096).reshape(-1, 1)\n",
    "        resp = gmm.predict_proba(grid)  # responsibilities per component\n",
    "        # find where |p0 - p1| is minimal\n",
    "        idx = int(np.argmin(np.abs(resp[:, 0] - resp[:, 1])))\n",
    "        t = float(grid[idx, 0])\n",
    "\n",
    "    return float(t)\n",
    "\n",
    "\n",
    "def robust_bg_threshold(intensities_bg, alpha=1e-3):\n",
    "    # Gaussian tail ⇒ k = Φ^{-1}(1-α)\n",
    "    from scipy.stats import norm\n",
    "    k = norm.ppf(1 - alpha)  # e.g., α=1e-3 → k≈3.09\n",
    "    mu = np.mean(intensities_bg)\n",
    "    sigma = np.std(intensities_bg, ddof=1)\n",
    "    return float(mu + k*sigma)\n",
    "\n",
    "def robust_bg_threshold_mad(intensities_bg, alpha=1e-3):\n",
    "    from scipy.stats import norm\n",
    "    k = norm.ppf(1 - alpha)\n",
    "    med = np.median(intensities_bg)\n",
    "    mad = np.median(np.abs(intensities_bg - med))\n",
    "    sigma_hat = 1.4826 * mad\n",
    "    return float(med + k*sigma_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0cec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_images():\n",
    "    img_numbers= [1,2,3,4,5,6]\n",
    "    all_images = []\n",
    "    for num in img_numbers:\n",
    "        channels = Read_image(num)\n",
    "        all_images.extend(channels)\n",
    "    return all_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a5d7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels read: 3\n",
      "Number of channels read: 3\n",
      "Number of channels read: 3\n",
      "Number of channels read: 3\n",
      "Number of channels read: 3\n",
      "Number of channels read: 3\n"
     ]
    }
   ],
   "source": [
    "all_images = read_all_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be33e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled = pooled_intensities(all_images, max_voxels=10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00eaa7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = thresholds_from_hist_methods(pooled)\n",
    "tau_otsu = taus[\"otsu\"]\n",
    "tau_yen = taus[\"yen\"]\n",
    "tau_triangle = taus[\"triangle\"]\n",
    "\n",
    "# or GMM:\n",
    "tau_gmm = gmm_bayes_threshold_1d(pooled,covariance_type='diag')\n",
    "\n",
    "# or background-anchored (need background samples):\n",
    "# bg = pooled_from_background_rois(...)\n",
    "# tau_bg = robust_bg_threshold(bg, alpha=1e-3)\n",
    "\n",
    "# 4) choose one τ (example: τ = tau_yen)\n",
    "tau = tau_yen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576d1a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'otsu': 111,\n",
       " 'yen': 11,\n",
       " 'triangle': 8,\n",
       " 'isodata': 111,\n",
       " 'li': 42.39745011937387}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a145ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.043956043956044, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_gmm, tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_name = [\"Endocardium\", \"Myocardium\", \"Nuclei\"]\n",
    "def plot_histogram(channels: list, threshold:int=50):\n",
    "    flattened_channels = [ch.flatten()[ch.flatten() > threshold] for ch in channels]\n",
    "    fig, axs = plt.subplots(len(flattened_channels), 1, figsize=(10, 6))\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.hist(flattened_channels[i], bins=256-threshold-1, alpha=0.7)\n",
    "        ax.set_title(channel_name[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e564e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(Read_image(1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad43557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_mkdir(folder_name:str)->str:\n",
    "    if not os.path.exists(os.path.join(folder_path, folder_name)):\n",
    "        os.makedirs(os.path.join(folder_path, folder_name))\n",
    "\n",
    "def recreate_img(img_number: int, threshold:int):\n",
    "    maybe_mkdir(f\"Thresholded_{threshold}\")\n",
    "    n=0\n",
    "    while True:\n",
    "        if os.path.isfile(os.path.join(folder_path,\"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\")):\n",
    "            img_path = os.path.join(folder_path,\"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\")\n",
    "            img = nib.load(img_path)\n",
    "            data = img.get_fdata().astype(np.uint8)\n",
    "            data[data < threshold] = 0\n",
    "            data = nib.Nifti1Image(data, img.affine, img.header)\n",
    "            nib.save(data, os.path.join(folder_path, f\"Thresholded_{threshold}\", \n",
    "                                        \"IF_Tr_%04d_%04d\"%(img_number,n)+\".nii.gz\"))\n",
    "\n",
    "        else:\n",
    "            break\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe81d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_img(1,43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3218ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_equalization(channels: list, method:str=\"global\"):\n",
    "    equalized_channels = []\n",
    "    for ch in channels:\n",
    "        # flatten -> equalize -> reshape back\n",
    "        if method == \"global\":\n",
    "            eq = exposure.equalize_hist(ch)  # global hist eq (range [0,1])\n",
    "        elif method == \"clahe\":\n",
    "            eq = exposure.equalize_adapthist(ch, clip_limit=0.03)  # CLAHE\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'global' or 'clahe'\")\n",
    "        equalized_channels.append(eq)\n",
    "    return equalized_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels read: 3\n"
     ]
    }
   ],
   "source": [
    "channels = Read_image(1)\n",
    "\n",
    "# Plot original histogram\n",
    "# plot_histogram(channels, -1)\n",
    "\n",
    "# Apply histogram equalization\n",
    "eq_channels = histogram_equalization(channels, method=\"global\")  # try \"global\" or \"clahe\"\n",
    "\n",
    "# Plot equalized histogram\n",
    "plot_histogram(eq_channels, -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IMAGE_ANALYSIS)",
   "language": "python",
   "name": "image_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
